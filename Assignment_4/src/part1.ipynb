{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 7) (210, 3)\n",
      "[[ 1.42097769e-01  2.15462437e-01  6.06017918e-05  3.04218199e-01\n",
      "   1.41701823e-01 -9.86151745e-01 -3.83577423e-01]\n",
      " [ 1.11880257e-02  8.22375713e-03  4.28515270e-01 -1.68624664e-01\n",
      "   1.97432229e-01 -1.78816620e+00 -9.22013487e-01]] [[1, 0, 0], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def Preprocess(FILE_NAME = \"seeds_dataset.txt\"):\n",
    "    fo = open(FILE_NAME, \"r\")\n",
    "    lines = fo.readlines()\n",
    "    fo.close()\n",
    "    features = []\n",
    "    labels = []\n",
    "    parseFeatures = lambda list_: [float(item) for item in list_] \n",
    "    convert2onehot = {'1': [1, 0, 0], '2': [0, 1, 0], '3': [0, 0, 1]}\n",
    "\n",
    "    for line in lines:\n",
    "        splitted_lines = line.split()\n",
    "        features.append(parseFeatures(splitted_lines[0:len(splitted_lines)-1]))\n",
    "        labels.append(convert2onehot[splitted_lines[len(splitted_lines)-1]])\n",
    "\n",
    "    features = np.array(features)\n",
    "\n",
    "    # Z-score normalisation\n",
    "    means = np.mean(features, axis=0)\n",
    "    standardDevs = np.std(features, axis=0)\n",
    "    normalizedFeatures = (features - means)/standardDevs\n",
    "\n",
    "    # Randomly shuffle\n",
    "    print(normalizedFeatures.shape, np.shape(labels))\n",
    "    print(normalizedFeatures[0:2], labels[0:2])\n",
    "    datas = list(zip(normalizedFeatures, labels))\n",
    "    random.shuffle(datas)\n",
    "    normalizedFeatures, labels = zip(*datas)\n",
    "\n",
    "    # Traaing test split\n",
    "    train_size = int(len(labels) * 0.8)\n",
    "    train_feats, train_labels = normalizedFeatures[0:train_size], labels[0:train_size]\n",
    "    l = len(normalizedFeatures)\n",
    "    test_feats, test_labels = normalizedFeatures[train_size:l], labels[train_size:l]\n",
    "\n",
    "    # Save the test and traing datas\n",
    "    np.savetxt(\"train_feats.txt\", train_feats)\n",
    "    np.savetxt(\"test_feats.txt\", test_feats)\n",
    "    np.savetxt(\"train_labels.txt\", train_labels, fmt=\"%d\")\n",
    "    np.savetxt(\"test_labels.txt\", test_labels, fmt=\"%d\")\n",
    "\n",
    "Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_loader():\n",
    "    train_feats = np.array(np.loadtxt(\"train_feats.txt\"))\n",
    "    test_feats = np.array(np.loadtxt(\"test_feats.txt\"))\n",
    "    train_labels = np.array(np.loadtxt(\"train_labels.txt\", dtype=int))\n",
    "    test_labels = np.array(np.loadtxt(\"test_labels.txt\", dtype=int))\n",
    "\n",
    "    train_mini_batched = [(train_feats[i:min(i+32, 168)], train_labels[i:min(i+32, 168)]) for i in range(0, 168, 32)]\n",
    "    test_mini_batched = [(train_feats[i:min(i+32, 42)], train_labels[i:min(i+32, 42)]) for i in range(0, 42, 32)]\n",
    "    return train_mini_batched, test_mini_batched, train_feats, train_labels, test_feats, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, p, y_true):\n",
    "        return -1 * np.sum(y_true * np.log(p))\n",
    "    def backward(self, p, y_true):\n",
    "        return -1 * (y_true * (1 / p)) / (np.shape(y_true)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dims, out_dims, activation=None):\n",
    "        self.W = np.random.randn(in_dims, out_dims)\n",
    "        self.b = np.random.randn(1, out_dims)\n",
    "        self.activation = activation\n",
    "        self.activation_func = None\n",
    "        self.activation_grad_func = None\n",
    "        if activation == \"relu\":\n",
    "            self.activation_func = np.vectorize(lambda x: 0.0 if x < 0 else x)\n",
    "            self.activation_grad_func = np.vectorize(lambda x: 0.0 if x <= 0 else 1.0)\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        Wx = np.matmul(x, self.W)\n",
    "        s = Wx + self.b\n",
    "\n",
    "        if self.activation == None:\n",
    "            return s\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            z = 1 / (1 + np.exp(-1 * s))\n",
    "            self.cache['z'] = z\n",
    "            return z\n",
    "        elif self.activation == \"relu\":\n",
    "            z = self.activation_func(s)\n",
    "            self.cache['z'] = z\n",
    "            return z\n",
    "\n",
    "    def backward(self, grads):\n",
    "        if self.activation == None:\n",
    "            ds = grads\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            z = self.cache['z']\n",
    "            ds = grads * z * (1 - z)\n",
    "        elif self.activation == \"relu\":\n",
    "            z = self.cache['z']\n",
    "            ds = grads * self.activation_grad_func(z)\n",
    "\n",
    "        db = np.expand_dims(np.sum(ds, axis=0), axis=0)\n",
    "        self.grads['db'] = db\n",
    "\n",
    "        da = ds\n",
    "        x = self.cache['x']\n",
    "        dW = np.sum(x[:,:, np.newaxis] * da[:, np.newaxis, :], axis = -3)\n",
    "        self.grads['dW'] = dW\n",
    "        dx = np.squeeze(np.matmul(da[:,np.newaxis, :], (self.W).T))\n",
    "\n",
    "        self.cache = {}\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        self.W = self.W - learning_rate * self.grads['dW']\n",
    "        self.b = self.b - learning_rate * self.grads['db']\n",
    "        self.grads = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    def __init__(self, num_classes):\n",
    "        self.I = np.identity(num_classes)\n",
    "        self.cache = {}\n",
    "        self.transpose_axis = np.array([0, 2, 1])\n",
    "\n",
    "    def forward(self, z):\n",
    "        maxim = (np.max(z, axis=1))[:, np.newaxis]\n",
    "        exps = np.exp(z - maxim)\n",
    "        sums = np.sum(exps, axis=1)[:, np.newaxis]\n",
    "        P = exps / sums # P = probablilities\n",
    "        self.cache['P'] = P[:, np.newaxis, :]\n",
    "        return P\n",
    "\n",
    "    def backward(self, dp):\n",
    "        P = self.cache['P']\n",
    "        self.cache = {}\n",
    "        local_grad = P * np.transpose(self.I-P, axes=self.transpose_axis)\n",
    "        return np.squeeze(np.matmul(dp[:, np.newaxis,:], local_grad))\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        pass # No weights to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weight_intializer(model): # Takes the model as input\n",
    "    # Iterates over eacvh layer\n",
    "    # And initializes between -1 and +1\n",
    "    for layer in model:\n",
    "        if type(layer) == Dense:\n",
    "            layer.W = np.random.rand(np.shape(layer.W)[0], np.shape(layer.W)[1]) * 2 - 1\n",
    "            layer.b = np.random.rand(np.shape(layer.b)[0], np.shape(layer.b)[1]) * 2 - 1\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_pass(model, x): # Does the forward pas over all the layers\n",
    "    for layer in model:\n",
    "        x = layer.forward(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_pass(model, loss_fn, probabilities, y, learning_rate):\n",
    "    # Computeloss\n",
    "    loss = loss_fn.forward(probabilities, y)\n",
    "\n",
    "    # Backpropagate and accumulate grads\n",
    "    dx = loss_fn.backward(probabilities, y)\n",
    "    for layer in reversed(model):\n",
    "        dx = layer.backward(dx)\n",
    "\n",
    "    # Update weights\n",
    "    for layer in model:\n",
    "        layer.update_weights(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(model, train_x, train_y, test_x, test_y):\n",
    "    # For training Data\n",
    "    predictions = Forward_pass(model, train_x)\n",
    "    train_correct = np.sum(np.argmax(predictions, axis=1) == np.argmax(train_y, axis=1))\n",
    "\n",
    "    # For testing Data\n",
    "    predictions = Forward_pass(model, test_x)\n",
    "    test_correct = np.sum(np.argmax(predictions, axis=1) == np.argmax(test_y, axis=1))\n",
    "    \n",
    "    return train_correct / np.shape(train_x)[0], test_correct / np.shape(test_x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(model, training_data, train_x, train_y, test_x, test_y, learning_rate=0.01, NUM_ITERS=200):\n",
    "    # Implement a simple mini batch SGD loop and \n",
    "    # train your neural network, using forward and backward passes.\n",
    "    loss_fn = crossEntropyLoss()\n",
    "    print(\"Intializing the Weights\")\n",
    "    Weight_intializer(model)\n",
    "\n",
    "    print(\"Entering the training loop\")\n",
    "    training_accuracy = []\n",
    "    testing_accuracy = []\n",
    "\n",
    "    for i in range(1, NUM_ITERS+1):\n",
    "        for (x, y) in training_data:\n",
    "            predicted_probabilities = Forward_pass(model, x)\n",
    "            Backward_pass(model, loss_fn, predicted_probabilities, y, learning_rate)\n",
    "        if (i%10 == 0):\n",
    "            print(\"Finished\", i, \"th iteration\")\n",
    "            _train, _test = Predict(model, train_x, train_y, test_x, test_y)\n",
    "            training_accuracy.append(_train)\n",
    "            testing_accuracy.append(_test)\n",
    "\n",
    "    return training_accuracy, testing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intializing the Weights\n",
      "Entering the training loop\n",
      "Finished 10 th iteration\n",
      "Finished 20 th iteration\n",
      "Finished 30 th iteration\n",
      "Finished 40 th iteration\n",
      "Finished 50 th iteration\n",
      "Finished 60 th iteration\n",
      "Finished 70 th iteration\n",
      "Finished 80 th iteration\n",
      "Finished 90 th iteration\n",
      "Finished 100 th iteration\n",
      "Finished 110 th iteration\n",
      "Finished 120 th iteration\n",
      "Finished 130 th iteration\n",
      "Finished 140 th iteration\n",
      "Finished 150 th iteration\n",
      "Finished 160 th iteration\n",
      "Finished 170 th iteration\n",
      "Finished 180 th iteration\n",
      "Finished 190 th iteration\n",
      "Finished 200 th iteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.4642857142857143, 0.5476190476190477),\n",
       " (0.6785714285714286, 0.7857142857142857),\n",
       " (0.8035714285714286, 0.8333333333333334),\n",
       " (0.8452380952380952, 0.9285714285714286),\n",
       " (0.8571428571428571, 0.9285714285714286),\n",
       " (0.8809523809523809, 0.9047619047619048),\n",
       " (0.8809523809523809, 0.9047619047619048),\n",
       " (0.8928571428571429, 0.9047619047619048),\n",
       " (0.8988095238095238, 0.9047619047619048),\n",
       " (0.8988095238095238, 0.9285714285714286),\n",
       " (0.9107142857142857, 0.9285714285714286),\n",
       " (0.9107142857142857, 0.9285714285714286),\n",
       " (0.9166666666666666, 0.9285714285714286),\n",
       " (0.9166666666666666, 0.9285714285714286),\n",
       " (0.9166666666666666, 0.9285714285714286),\n",
       " (0.9166666666666666, 0.9285714285714286),\n",
       " (0.9226190476190477, 0.9285714285714286),\n",
       " (0.9226190476190477, 0.9285714285714286),\n",
       " (0.9285714285714286, 0.9285714285714286),\n",
       " (0.9285714285714286, 0.9285714285714286)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, _, train_x, train_y, test_x, test_y = Data_loader()\n",
    "\n",
    "model = (Dense(7, 32, \"sigmoid\"), Dense(32, 3), softmax(3))\n",
    "train_accu, test_accu = Training(model, training_data, train_x, train_y, test_x, test_y)\n",
    "list(zip(train_accu, test_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intializing the Weights\n",
      "Entering the training loop\n",
      "Finished 10 th iteration\n",
      "Finished 20 th iteration\n",
      "Finished 30 th iteration\n",
      "Finished 40 th iteration\n",
      "Finished 50 th iteration\n",
      "Finished 60 th iteration\n",
      "Finished 70 th iteration\n",
      "Finished 80 th iteration\n",
      "Finished 90 th iteration\n",
      "Finished 100 th iteration\n",
      "Finished 110 th iteration\n",
      "Finished 120 th iteration\n",
      "Finished 130 th iteration\n",
      "Finished 140 th iteration\n",
      "Finished 150 th iteration\n",
      "Finished 160 th iteration\n",
      "Finished 170 th iteration\n",
      "Finished 180 th iteration\n",
      "Finished 190 th iteration\n",
      "Finished 200 th iteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9166666666666666, 0.9285714285714286),\n",
       " (0.9404761904761905, 0.9285714285714286),\n",
       " (0.9464285714285714, 0.9285714285714286),\n",
       " (0.9583333333333334, 0.9285714285714286),\n",
       " (0.9702380952380952, 0.9285714285714286),\n",
       " (0.9702380952380952, 0.9285714285714286),\n",
       " (0.9761904761904762, 0.9285714285714286),\n",
       " (0.9821428571428571, 0.9285714285714286),\n",
       " (0.9821428571428571, 0.9285714285714286),\n",
       " (0.9880952380952381, 0.9285714285714286),\n",
       " (0.9880952380952381, 0.9285714285714286),\n",
       " (0.9880952380952381, 0.9285714285714286),\n",
       " (0.9880952380952381, 0.9523809523809523),\n",
       " (0.9880952380952381, 0.9523809523809523),\n",
       " (0.9880952380952381, 0.9523809523809523),\n",
       " (0.9880952380952381, 0.9523809523809523),\n",
       " (0.9940476190476191, 0.9523809523809523),\n",
       " (0.9940476190476191, 0.9523809523809523),\n",
       " (0.9940476190476191, 0.9523809523809523),\n",
       " (0.9940476190476191, 0.9523809523809523)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (Dense(7, 64, \"relu\"), Dense(64, 32, \"relu\"), Dense(32, 3), softmax(3))\n",
    "train_accu, test_accu = Training(model, training_data, train_x, train_y, test_x, test_y)\n",
    "list(zip(train_accu, test_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model = (Dense(7, 8, \"relu\"), Dense(8, 6, \"relu\"), Dense(6, 3), softmax(3))\n",
    "\n",
    "    w1 = np.zeros((7, 8))\n",
    "    for i in range(0, 7):\n",
    "        for j in range(0, 8):\n",
    "            w1[i][j] = 1/(i + j + 1)\n",
    "\n",
    "    w2 = np.zeros((8, 6))\n",
    "    for i in range(0, 8):\n",
    "        for j in range(0, 6):\n",
    "            w2[i][j] = 1/(i + j + 1)\n",
    "\n",
    "    w3 = np.zeros((6, 3))\n",
    "    for i in range(0, 6):\n",
    "        for j in range(0, 3):\n",
    "            w3[i][j] = 1/(i + j + 1)\n",
    "\n",
    "    b1 = np.zeros((1, 8))\n",
    "    for j in range(0, 8):\n",
    "        b1[0][j] = 1/(j + 1)\n",
    "\n",
    "    b2 = np.zeros((1, 6))\n",
    "    for j in range(0, 6):\n",
    "        b2[0][j] = 1/(j + 1)\n",
    "\n",
    "    b3 = np.zeros((1, 3))\n",
    "    for j in range(0, 3):\n",
    "        b3[0][j] = 1/(j + 1)\n",
    "\n",
    "    print(w1, \"\\n\\n\", w2, \"\\n\\n\", w3, \"\\n\\n\", b1, \"\\n\\n\", b2, \"\\n\\n\", b3, \"\\n\\n\")\n",
    "    model[0].W = w1\n",
    "    model[0].b = b1\n",
    "    model[1].W = w2\n",
    "    model[1].b = b2\n",
    "    model[2].W = w3\n",
    "    model[2].b = b3\n",
    "    \n",
    "    x = [[ 1.42097769e-01, 2.15462437e-01, 6.06017918e-05, 3.04218199e-01,\n",
    "       1.41701823e-01, -9.86151745e-01, -3.83577423e-01], \n",
    "     [ 1.11880257e-02, 8.22375713e-03, 4.28515270e-01, -1.68624664e-01,\n",
    "       1.97432229e-01, -1.78816620e+00, -9.22013487e-01]]\n",
    "    x = np.array(x)\n",
    "    y = [[1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "    loss_fn = crossEntropyLoss()\n",
    "    predicted_probabilities = Forward_pass(model, x)     \n",
    "\n",
    "    dx = loss_fn.backward(predicted_probabilities, y)\n",
    "    for layer in reversed(model):\n",
    "        print(dx.shape)\n",
    "        dx = layer.backward(dx)\n",
    "\n",
    "    return model[0].grads, model[1].grads, model[2].grads \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
